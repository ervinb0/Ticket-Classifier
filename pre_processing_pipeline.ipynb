{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "from langdetect import detect, DetectorFactory\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "\n",
    "# Getting the list of stopwords\n",
    "stop_words = set(stopwords.words('english'))\n",
    "\n",
    "def remove_noise(text):\n",
    "    # Remove special characters and numbers\n",
    "    #text = re.sub(r'[^a-zA-Z\\s?]', '', str(text)) #keeps question marks\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text))\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_emojis_and_links(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                           u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                           u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                           u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                           u\"\\U0001F1E0-\\U0001F1FF\"  # flags (iOS)\n",
    "                           \"]+\", flags=re.UNICODE)\n",
    "    link_pattern = re.compile(r'http\\S+|www\\S+')\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = link_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en' \n",
    "    except:\n",
    "        return False  # Return False if detection fails\n",
    "\n",
    "# Applying the language detection function to filter only English records\n",
    "# df = df[df['text'].apply(is_english)]\n",
    "\n",
    "# normalization\n",
    "df['cleaned_text'].str.lower()\n",
    "\n",
    "\n",
    "\n",
    "# Function to remove stopwords\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "# Applying the function to the cleaned_text column\n",
    "# df['cleaned_text'].apply(remove_stopwords)\n",
    "# df['cleaned_text'].head()\n",
    "\n",
    "\n",
    "\n",
    "# df['tokenized_text'] = df['cleaned_text'].apply(nltk.word_tokenize)\n",
    "# df['tokenized_text'].head()\n",
    "\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "# Initialize the stemmer\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Function to stem the tokens\n",
    "def stem_tokens(tokens):\n",
    "    return [stemmer.stem(token) for token in tokens]\n",
    "\n",
    "# Apply the stemming function to the tokenized_text column\n",
    "df['stemmed_text'] = df['tokenized_text'].apply(stem_tokens)\n",
    "df['stemmed_text'].head()\n",
    "\n",
    "#text preprocessing function\n",
    "def text_preprocessing(text):\n",
    "    # Implement any preprocessing steps here, e.g., removing punctuation, lowercasing, etc.\n",
    "    return text.lower()\n",
    "\n",
    "# Load your dataset\n",
    "# Assuming df['text'] contains the text and df['label'] the target\n",
    "df = pd.DataFrame({\n",
    "    'text': [\"Sample issue text 1\", \"Sample issue text 2\"],  # replace with actual text\n",
    "    'label': [\"question\", \"bug\"]  # replace with actual labels\n",
    "})\n",
    "\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "\n",
    "# Split data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Build the pipeline\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(preprocessor=text_preprocessing)),  # Feature extraction with text preprocessing\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))  # Random Forest model\n",
    "])\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import classification_report\n",
    "from langdetect import detect, DetectorFactory\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "\n",
    "DetectorFactory.seed = 0\n",
    "\n",
    "nltk.download('stopwords')\n",
    "nltk.download('punkt_tab')\n",
    "nltk.download('punkt')\n",
    "\n",
    "stop_words = set(stopwords.words('english'))\n",
    "stemmer = PorterStemmer()\n",
    "\n",
    "# Define each preprocessing function\n",
    "\n",
    "def remove_noise(text):\n",
    "    # Remove special characters and numbers\n",
    "    text = re.sub(r'[^a-zA-Z\\s]', '', str(text))\n",
    "    # Remove extra spaces\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def remove_emojis_and_links(text):\n",
    "    emoji_pattern = re.compile(\"[\"\n",
    "                               u\"\\U0001F600-\\U0001F64F\"  # emoticons\n",
    "                               u\"\\U0001F300-\\U0001F5FF\"  # symbols & pictographs\n",
    "                               u\"\\U0001F680-\\U0001F6FF\"  # transport & map symbols\n",
    "                               u\"\\U0001F1E0-\\U0001F1FF\"  # flags\n",
    "                               \"]+\", flags=re.UNICODE)\n",
    "    link_pattern = re.compile(r'http\\S+|www\\S+')\n",
    "    text = emoji_pattern.sub(r'', text)\n",
    "    text = link_pattern.sub(r'', text)\n",
    "    return text\n",
    "\n",
    "def is_english(text):\n",
    "    try:\n",
    "        return detect(text) == 'en'\n",
    "    except:\n",
    "        return False  # Return False if detection fails\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    words = text.split()\n",
    "    filtered_words = [word for word in words if word.lower() not in stop_words]\n",
    "    return ' '.join(filtered_words)\n",
    "\n",
    "def stem_text(text):\n",
    "    tokens = word_tokenize(text)\n",
    "    stemmed_tokens = [stemmer.stem(token) for token in tokens]\n",
    "    return ' '.join(stemmed_tokens)\n",
    "\n",
    "def text_preprocessing(text):\n",
    "    # Check if text is in English\n",
    "    if not is_english(text):\n",
    "        return \"\"\n",
    "    \n",
    "    # Apply all preprocessing steps\n",
    "    text = text.lower()                         # Normalization\n",
    "    text = remove_noise(text)                   # Remove noise\n",
    "    text = remove_emojis_and_links(text)        # Remove emojis and links\n",
    "    text = remove_stopwords(text)               # Remove stopwords\n",
    "    text = stem_text(text)                      # Apply stemming\n",
    "    \n",
    "    return text\n",
    "\n",
    "# Build the pipeline with the consolidated preprocessing\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(preprocessor=text_preprocessing)),  # Feature extraction with text preprocessing\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42))  # Random Forest model\n",
    "])\n",
    "\"\"\"\n",
    "# Split data (assuming df['text'] and df['label'] exist)\n",
    "X = df['text']\n",
    "y = df['label']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train the model\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model\n",
    "y_pred = pipeline.predict(X_test)\n",
    "print(classification_report(y_test, y_pred))\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                        stemmed_text issue_label\n",
      "0  ['tilesrcrect', 'null', 'entitiesldtk', 'sampl...         bug\n",
      "1  ['updat', 'blog', 'link', 'sef', 'site', 'desc...         bug\n",
      "2  ['parser', 'can', 'not', 'properli', 'distingu...         bug\n",
      "3  ['row', 'ad', 'snackbar', 'visibl', 'grid', 's...         bug\n",
      "4  ['crash', 'tcpwriteonsocket', 'crash', 'flush'...         bug\n",
      "Features: (59973,), Labels: (59973,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# Load the dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Select only the columns you need\n",
    "df = df[['stemmed_text', 'issue_label']]\n",
    "\n",
    "# Drop any rows with missing values in these columns, if necessary\n",
    "df.dropna(subset=['stemmed_text', 'issue_label'], inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df['stemmed_text']  # Text column\n",
    "y = df['issue_label']    # Label column\n",
    "\n",
    "# Check the data to ensure it loaded correctly\n",
    "print(df.head())\n",
    "print(f\"Features: {X.shape}, Labels: {y.shape}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the dataset\n",
    "df = pd.read_csv('dataset.csv')\n",
    "\n",
    "# Select only the columns you need\n",
    "df = df[['stemmed_text', 'issue_label']]\n",
    "\n",
    "# Drop any rows with missing values in these columns, if necessary\n",
    "df.dropna(subset=['stemmed_text', 'issue_label'], inplace=True)\n",
    "\n",
    "# Separate features and labels\n",
    "X = df['stemmed_text']  # Text column\n",
    "y = df['issue_label']   # Label column\n",
    "\n",
    "# Split data into train, validation, and test sets\n",
    "X_train_val, X_test, y_train_val, y_test = train_test_split(\n",
    "    X, y, test_size=0.2, random_state=42, stratify=y\n",
    ")\n",
    "\n",
    "X_train, X_val, y_train, y_val = train_test_split(\n",
    "    X_train_val, y_train_val, test_size=0.2, random_state=42, stratify=y_train_val\n",
    ")\n",
    "\n",
    "pipeline = Pipeline([\n",
    "    ('vectorizer', TfidfVectorizer(preprocessor=text_preprocessing)),\n",
    "    ('classifier', RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced'))\n",
    "])\n",
    "\n",
    "# Train the model on the training set\n",
    "pipeline.fit(X_train, y_train)\n",
    "\n",
    "# Evaluate the model on the validation set\n",
    "y_val_pred = pipeline.predict(X_val)\n",
    "print(\"Validation Set Classification Report:\\n\", classification_report(y_val, y_val_pred))\n",
    "\n",
    "# Evaluate the model on the test set\n",
    "y_test_pred = pipeline.predict(X_test)\n",
    "print(\"Test Set Classification Report:\\n\", classification_report(y_test, y_test_pred))\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "project_env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
